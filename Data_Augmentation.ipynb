{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Augmentation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONC83b3eFi4Q"
      },
      "source": [
        "#  Way one: Increase by copy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pb6t-TnGFZyG"
      },
      "source": [
        "\n",
        "from sklearn.utils import resample\n",
        "df = pd.concat([train_x, train_labels], axis=1)\n",
        "# Separate majority and minority classes\n",
        "df_majority = df[train_labels==0]\n",
        "df_minority1 = df[train_labels == 1]\n",
        "df_minority2 = df[train_labels == 2]\n",
        "df_minority3 = df[train_labels == 3]\n",
        "df_minority4 = df[train_labels == 4]\n",
        "\n",
        "\n",
        "# Upsample minority class\n",
        "df_minority_upsampled1 = resample(df_minority1, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=len(df_majority),    # to match majority class\n",
        "                                 random_state=1234) # reproducible results\n",
        "\n",
        "df_minority_upsampled2 = resample(df_minority2, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=len(df_majority),    # to match majority class\n",
        "                                 random_state=1234) # reproducible results\n",
        "\n",
        "df_minority_upsampled3 = resample(df_minority3, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=len(df_majority),    # to match majority class\n",
        "                                 random_state=1234) # reproducible results\n",
        "\n",
        "df_minority_upsampled4 = resample(df_minority4, \n",
        "                                 replace=True,     # sample with replacement\n",
        "                                 n_samples=len(df_majority),    # to match majority class\n",
        "                                 random_state=1234) # reproducible results\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL50M1BMF3jT"
      },
      "source": [
        "# By elastic distortion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzIC2heuF8lh"
      },
      "source": [
        "from scipy.ndimage.filters import gaussian_filter\n",
        "from scipy.ndimage.interpolation import map_coordinates\n",
        "\n",
        "def elastic_transform(image, alpha_range, sigma, random_state=None):\n",
        "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
        "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
        "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
        "       Proc. of the International Conference on Document Analysis and\n",
        "       Recognition, 2003.\n",
        "       \n",
        "   # Arguments\n",
        "       image: Numpy array with shape (height, width, channels). \n",
        "       alpha_range: Float for fixed value or [lower, upper] for random value from uniform distribution.\n",
        "           Controls intensity of deformation.\n",
        "       sigma: Float, sigma of gaussian filter that smooths the displacement fields.\n",
        "       random_state: `numpy.random.RandomState` object for generating displacement fields.\n",
        "    \"\"\"\n",
        "    \n",
        "    if random_state is None:\n",
        "        random_state = np.random.RandomState(None)\n",
        "        \n",
        "    if np.isscalar(alpha_range):\n",
        "        alpha = alpha_range\n",
        "    else:\n",
        "        alpha = np.random.uniform(low=alpha_range[0], high=alpha_range[1])\n",
        "\n",
        "    shape = image.shape\n",
        "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
        "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma) * alpha\n",
        "\n",
        "    x, y, z = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), np.arange(shape[2]), indexing='ij')\n",
        "    indices = np.reshape(x+dx, (-1, 1)), np.reshape(y+dy, (-1, 1)), np.reshape(z, (-1, 1))\n",
        "\n",
        "    return map_coordinates(image, indices, order=1, mode='reflect').reshape(shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}