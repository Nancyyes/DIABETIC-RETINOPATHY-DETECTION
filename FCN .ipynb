{
  "cells": [
    {
      "cell_type": "code",
      "source": "!pip install tensorflow\n",
      "metadata": {
        "tags": [],
        "cell_id": "00000-a5967f6f-1a04-4c8a-b62d-bdeb6f9009c9",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d690a92f",
        "execution_millis": 40228,
        "execution_start": 1617405849499,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting tensorflow\n  Downloading tensorflow-2.5.0rc0-cp39-cp39-manylinux2010_x86_64.whl (454.3 MB)\n\u001b[K     |███████████████████████▏        | 328.6 MB 77.2 MB/s eta 0:00:02IOPub data rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_data_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n\u001b[K     |████████████████████████████████| 454.3 MB 1.2 kB/s \n\u001b[?25hCollecting astunparse~=1.6.3\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nRequirement already satisfied: protobuf>=3.9.2 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from tensorflow) (3.15.6)\nCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp39-cp39-manylinux1_x86_64.whl (4.4 MB)\n\u001b[K     |████████████████████████████████| 4.4 MB 44.5 MB/s \n\u001b[?25hRequirement already satisfied: typing-extensions~=3.7.4 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from tensorflow) (3.7.4.3)\nCollecting absl-py~=0.10\n  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n\u001b[K     |████████████████████████████████| 129 kB 84.7 MB/s \n\u001b[?25hCollecting flatbuffers~=1.12.0\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\nCollecting numpy~=1.19.2\n  Downloading numpy-1.19.5-cp39-cp39-manylinux2010_x86_64.whl (14.9 MB)\n\u001b[K     |████████████████████████████████| 14.9 MB 98.5 MB/s \n\u001b[?25hCollecting termcolor~=1.1.0\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\nCollecting wrapt~=1.12.1\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\nRequirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.9/site-packages (from tensorflow) (0.36.2)\nCollecting gast==0.4.0\n  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\nCollecting google-pasta~=0.2\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n\u001b[K     |████████████████████████████████| 57 kB 12.9 MB/s \n\u001b[?25hCollecting keras-preprocessing~=1.1.2\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n\u001b[K     |████████████████████████████████| 42 kB 3.6 MB/s \n\u001b[?25hCollecting keras-nightly~=2.5.0.dev\n  Downloading keras_nightly-2.5.0.dev2021032900-py2.py3-none-any.whl (1.2 MB)\n\u001b[K     |████████████████████████████████| 1.2 MB 91.1 MB/s \n\u001b[?25hRequirement already satisfied: six~=1.15.0 in /shared-libs/python3.9/py-core/lib/python3.9/site-packages (from tensorflow) (1.15.0)\nCollecting tf-estimator-nightly==2.5.0.dev2021032501\n  Downloading tf_estimator_nightly-2.5.0.dev2021032501-py2.py3-none-any.whl (462 kB)\n\u001b[K     |████████████████████████████████| 462 kB 82.4 MB/s \n\u001b[?25hCollecting opt-einsum~=3.3.0\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n\u001b[K     |████████████████████████████████| 65 kB 11.6 MB/s \n\u001b[?25hCollecting tensorboard~=2.4\n  Downloading tensorboard-2.4.1-py3-none-any.whl (10.6 MB)\n\u001b[K     |████████████████████████████████| 10.6 MB 63.2 MB/s \n\u001b[?25hCollecting grpcio~=1.34.0\n  Downloading grpcio-1.34.1-cp39-cp39-manylinux2014_x86_64.whl (4.0 MB)\n\u001b[K     |████████████████████████████████| 4.0 MB 46.7 MB/s \n\u001b[?25hCollecting markdown>=2.6.8\n  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n\u001b[K     |████████████████████████████████| 97 kB 18.9 MB/s \n\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\nRequirement already satisfied: requests<3,>=2.21.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from tensorboard~=2.4->tensorflow) (2.25.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from tensorboard~=2.4->tensorflow) (1.27.1)\nCollecting tensorboard-plugin-wit>=1.6.0\n  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n\u001b[K     |████████████████████████████████| 781 kB 80.3 MB/s \n\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /root/venv/lib/python3.9/site-packages (from tensorboard~=2.4->tensorflow) (49.2.1)\nCollecting werkzeug>=0.11.15\n  Downloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\n\u001b[K     |████████████████████████████████| 298 kB 91.6 MB/s \n\u001b[?25hRequirement already satisfied: requests-oauthlib>=0.7.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.3.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.26.4)\nRequirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.2.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.8)\nRequirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /shared-libs/python3.9/py/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.7.2)\nRequirement already satisfied: oauthlib>=3.0.0 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (3.1.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /shared-libs/python3.9/py/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.8)\nBuilding wheels for collected packages: termcolor, wrapt\n  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4829 sha256=1b2d7fe3b98f6dbce6b6a1340e2eb9157eef78d82e6bdb02ae4a8088500a0327\n  Stored in directory: /root/.cache/pip/wheels/b6/0d/90/0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp39-cp39-linux_x86_64.whl size=33476 sha256=4eed1c71412a5014debe546e2d483c22d8d1265d70f44ca2f217718139d53f7c\n  Stored in directory: /root/.cache/pip/wheels/98/23/68/efe259aaca055e93b08e74fbe512819c69a2155c11ba3c0f10\nSuccessfully built termcolor wrapt\nInstalling collected packages: astunparse, numpy, h5py, absl-py, flatbuffers, termcolor, wrapt, gast, google-pasta, keras-preprocessing, keras-nightly, tf-estimator-nightly, opt-einsum, markdown, google-auth-oauthlib, grpcio, tensorboard-plugin-wit, werkzeug, tensorboard, tensorflow\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.20.1\n    Not uninstalling numpy at /shared-libs/python3.9/py/lib/python3.9/site-packages, outside environment /root/venv\n    Can't uninstall 'numpy'. No files were found to uninstall.\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.2.1\n    Not uninstalling h5py at /shared-libs/python3.9/py/lib/python3.9/site-packages, outside environment /root/venv\n    Can't uninstall 'h5py'. No files were found to uninstall.\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.36.1\n    Not uninstalling grpcio at /shared-libs/python3.9/py/lib/python3.9/site-packages, outside environment /root/venv\n    Can't uninstall 'grpcio'. No files were found to uninstall.\nSuccessfully installed absl-py-0.12.0 astunparse-1.6.3 flatbuffers-1.12 gast-0.4.0 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.34.1 h5py-3.1.0 keras-nightly-2.5.0.dev2021032900 keras-preprocessing-1.1.2 markdown-3.3.4 numpy-1.19.5 opt-einsum-3.3.0 tensorboard-2.4.1 tensorboard-plugin-wit-1.8.0 tensorflow-2.5.0rc0 termcolor-1.1.0 tf-estimator-nightly-2.5.0.dev2021032501 werkzeug-1.0.1 wrapt-1.12.1\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 18
    },
    {
      "cell_type": "code",
      "source": "!pip install opencv-python==4.5.1.48",
      "metadata": {
        "tags": [],
        "cell_id": "00001-1c91f671-2a4d-45a6-8847-48ecafaee43b",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "7cf01600",
        "execution_millis": 2564,
        "execution_start": 1617406037052,
        "deepnote_cell_type": "code"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: opencv-python==4.5.1.48 in /root/venv/lib/python3.9/site-packages (4.5.1.48)\nRequirement already satisfied: numpy>=1.19.3 in /root/venv/lib/python3.9/site-packages (from opencv-python==4.5.1.48) (1.19.5)\n\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.0.1 is available.\nYou should consider upgrading via the '/root/venv/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00000-8d2d1640-e68e-48a8-b1f0-6e34b775ab42",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "771f71aa",
        "execution_millis": 1,
        "execution_start": 1617406051875,
        "deepnote_cell_type": "code"
      },
      "source": "from tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator ## NN \nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport os\nimport pandas as pd\nimport glob\nfrom PIL import Image",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-03449f33-ef87-4867-8eba-a274608619a0",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "1c035c3a",
        "execution_millis": 11815,
        "execution_start": 1617406101804,
        "deepnote_cell_type": "code"
      },
      "source": "main_dir = \"/work/Data\"\ntrain_dir = os.path.join(main_dir,\"/work/Data/train_data\")\n# test_dir = os.path.join(main_dir,\"test_images\")\ntrain_data_image = !ls /datasets/trainimage\n",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# The clinicians have rated each image for the severity of diabetic retinopathy on a scale of 0 to 4. It is a multi-class problem with 5 target classes\n0 - No DR, 1 - Mild, 2 - Moderate 3 - Severe, 4 - Proliferative DR",
      "metadata": {
        "tags": [],
        "cell_id": "00002-91e6bb59-af55-41e2-a719-82e2232a19dd",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-039abb45-be31-4afa-8193-94c11ba2daa1",
        "deepnote_cell_type": "code"
      },
      "source": "Train_data = pd.read_csv(\"../input/aptos2019-blindness-detection/train.csv\")\nTest_data = pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\ntrain_0 = np.count_nonzero(Train_data[\"diagnosis\"] == 0)\ntrain_1 = np.count_nonzero(Train_data[\"diagnosis\"] == 1)\ntrain_2 = np.count_nonzero(Train_data[\"diagnosis\"] == 2)\ntrain_3 = np.count_nonzero(Train_data[\"diagnosis\"] == 3)\ntrain_4 = np.count_nonzero(Train_data[\"diagnosis\"] == 4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-866110b9-3b00-4870-b3a7-85a4cdf2c8c3",
        "deepnote_cell_type": "code"
      },
      "source": "# data split\nfrom sklearn.model_selection import train_test_split\n# load image and convert to and from NumPy array\nfrom PIL import Image\nfrom numpy import argmax\nimport numpy as np\nimport cv2\nfrom PIL import Image\ntrain_x = Train_data[\"id_code\"]\ntrain_labels = Train_data[\"diagnosis\"]\n\ntrain_pic_path= [os.path.join(train_dir, filename)for filename in train_x]\ntrain_image = []\nfor i, img_path in enumerate(train_pic_path):\n    img_path = img_path + \".png\"\n    train_image.append(img_path)\nprint(len(train_image))\n\ndata_out = [0,1,2,3,4]\nlabel_int =  dict((c, i) for i, c in enumerate(data_out))\nint_labe = dict((i, c) for i, c in enumerate(data_out))\ninterger_encoded = [label_int[char] for char in train_labels]\n\nonehot_encoded = []\nfor value in interger_encoded:\n    labe_n = [0 for _ in range(len(data_out))]\n    labe_n[value] = 1\n    onehot_encoded.append(labe_n)\ntrain_onehot_labe = onehot_encoded\n\ndata_frame_work = pd.DataFrame({\"Train_label\": train_onehot_labe,\n                               \"Train_image\": train_image,\n                               \"Triain_id\": train_x},\n                              columns=[\"Train_label\",\"Train_image\",\"Triain_id\"])\n\n\nValida_dataset = data_frame_work.sample(n=500)\nTrain_dataset = data_frame_work.drop(Valida_dataset.index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": " # Image Analysis",
      "metadata": {
        "tags": [],
        "cell_id": "00005-dfab3c8d-7738-4b2d-962a-ce31058f8812",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00006-d7402e75-3557-41c8-ab1b-8bd0626582e2",
        "deepnote_cell_type": "code"
      },
      "source": "\ndef image_analysis(dataframe, path):\n    width_range = []\n    height_range = []\n    for i in range(dataframe.shape[0]):\n        img = cv2.imread(path+dataframe.iloc[i]['id_code']+'.png')\n        height, width, _ = img.shape\n        width_range.append(width)\n        height_range.append(height)\n    return width_range, height_range\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00007-4a7aca1d-9a00-4c8e-8b91-73ffbb1250f6",
        "deepnote_cell_type": "code"
      },
      "source": "width_range, height_range = image_analysis(Train_data, \"../input/aptos2019-blindness-detection/train_images/\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00008-f0a100c9-2cff-45bd-a5b8-d4eccde92b22",
        "deepnote_cell_type": "code"
      },
      "source": "# check image ratio of width and height\nimport seaborn as sns\nratio_list = []\n\nfor i,j in zip(width_range,height_range):\n    ratio = i/j\n    ratio_list.append(ratio)\n\nsns.distplot(ratio_list, hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00009-6b19ab54-7cc6-4075-b02c-2422866d8957",
        "deepnote_cell_type": "code"
      },
      "source": "hh = max(height_range)\nww = max(width_range)\ncc = 3\ncolor = (0,0,0) # add 0 which is the same with backgroup\nresult = np.full((hh,ww,cc), color, dtype=np.uint8)\nnew_image = []\ndef pad_max(image_group): \n    '''\n    image_group: list of image path\n    '''\n    for image in image_group:\n        imge = cv2.imread(image)\n        ht, wd, cc = imge.shape\n        \n        # compute center offset\n        xx = (ww - wd) // 2\n        yy = (hh - ht) // 2\n        \n        # copy img image into center of result image\n        result[yy:yy+ht, xx:xx+wd] = imge\n        \n        new_image.append(imge)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00010-1addb604-79dd-4be2-a60b-98f515e4ae77",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b8ac4fcd",
        "execution_millis": 2498,
        "execution_start": 1617397179698,
        "deepnote_cell_type": "code"
      },
      "source": "train_data = !ls /datasets/trainimage \n",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00012-044cc23e-c664-430a-97e7-2fd56baea64e",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "1c986a8a",
        "execution_start": 1617397200504,
        "execution_millis": 5,
        "deepnote_cell_type": "code"
      },
      "source": "# width, height check\ndef image_analysis(dataframe, path): \n    width_range = []\n    height_range = []\n    for i in range(dataframe.shape[0]):\n        img = cv2.imread(path+dataframe.iloc[i]['id_code']+'.png')\n        height, width, _ = img.shape\n        width_range.append(width)\n        height_range.append(height)\n    return width_range, height_range\n\ndef wid_heigh(image_group):\n    width_range = []\n    height_range = []\n    for i in image_group:\n        height, width, _ = i.shape\n        width_range.append(width)\n        height_range.append(height)\n    return width_range, height_range \n\n# Check all png have the same size\ndef ckeckList(lst):\n  \n    ele = lst[0]\n    chk = True\n      \n    # Comparing each element with first item \n    for item in lst:\n        if ele != item:\n            chk = False\n            break;\n              \n    if (chk == True): print(\"Equal\")\n    else: print(\"Not equal\")   \n        \ndef pad_max(image_group): \n    '''\n    image_group: list of image path\n    '''\n    for image in image_group:\n        imge = cv2.imread(image)\n        ht, wd, cc = imge.shape\n        \n        # compute center offset\n        xx = (ww - wd) // 2\n        yy = (hh - ht) // 2\n        \n        # copy img image into center of result image\n        result[yy:yy+ht, xx:xx+wd] = imge\n        \n        new_image.append(result)\n        ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "3663"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-f3a1b6b4-9e62-4db5-9125-bc7a78e22086",
        "deepnote_cell_type": "code"
      },
      "source": "width_range, height_range = image_analysis(Train_data, \"../input/aptos2019-blindness-detection/train_images/\")\n# check image ratio of width and height\nimport seaborn as sns\nratio_list = []\n\nfor i,j in zip(width_range,height_range):\n    ratio = i/j\n    ratio_list.append(ratio)\n\nsns.distplot(ratio_list, hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\n    \n# add 0 to images to make it same size\nhh = max(height_range)\nww = max(width_range)\ncc = 3\ncolor = (0,0,0) # add 0 which is the same with backgroup\nresult = np.full((hh,ww,cc), color, dtype=np.uint8)\nnew_image = []\n  \n# check results \nimage_list = Train_dataset[\"Train_image\"][1:200]\npad_max(image_list)\nwidth_list, height_list  = wid_heigh(new_image)\nlst1 = height_list\nckeckList(lst1)\nlst2 = width_list\nckeckList(lst2)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Model Part",
      "metadata": {
        "tags": [],
        "cell_id": "00013-69b0df47-7bf0-4228-8e2e-f9d1fd68c6f5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00014-02a4dd5a-759e-438f-b69f-de3c6d47ce3e",
        "deepnote_cell_type": "code"
      },
      "source": "import tensorflow as tf\n\ndef FCN_model(len_classes=5, dropout_rate=0.2):\n    \n    input = tf.keras.layers.Input(shape=(None, None, 3))\n\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1)(input)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # x = tf.keras.layers.MaxPooling2D()(x)\n\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # x = tf.keras.layers.MaxPooling2D()(x)\n\n    x = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2)(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # x = tf.keras.layers.MaxPooling2D()(x)\n\n    x = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=2)(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # x = tf.keras.layers.MaxPooling2D()(x)\n\n    x = tf.keras.layers.Conv2D(filters=512, kernel_size=3, strides=2)(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # Uncomment the below line if you're using dense layers\n    # x = tf.keras.layers.GlobalMaxPooling2D()(x)\n\n    # Fully connected layer 1\n    # x = tf.keras.layers.Dropout(dropout_rate)(x)\n    # x = tf.keras.layers.BatchNormalization()(x)\n    # x = tf.keras.layers.Dense(units=64)(x)\n    # x = tf.keras.layers.Activation('relu')(x)\n\n    # Fully connected layer 1\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=1, strides=1)(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # Fully connected layer 2\n    # x = tf.keras.layers.Dropout(dropout_rate)(x)\n    # x = tf.keras.layers.BatchNormalization()(x)\n    # x = tf.keras.layers.Dense(units=len_classes)(x)\n    # predictions = tf.keras.layers.Activation('softmax')(x)\n\n    # Fully connected layer 2\n    x = tf.keras.layers.Conv2D(filters=len_classes, kernel_size=1, strides=1)(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.GlobalMaxPooling2D()(x)\n    predictions = tf.keras.layers.Activation('softmax')(x)\n\n    model = tf.keras.Model(inputs=input, outputs=predictions)\n    \n    print(model.summary())\n    print(f'Total number of layers: {len(model.layers)}')\n\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Generator Part",
      "metadata": {
        "tags": [],
        "cell_id": "00015-b3d55d1c-5bd8-4f15-a843-4995e6724085",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00015-2378e663-65be-46a6-99b4-0cf5208207ae",
        "deepnote_cell_type": "code"
      },
      "source": "class Generator(tf.keras.utils.Sequence):\n\n    def __init__(self, DATASET_PATH, BATCH_SIZE=32, shuffle_images=True, image_min_side=24):\n        \"\"\" Initialize Generator object.\n        Args \n            DATASET_PATH           : Path to data dataframe\n            BATCH_SIZE             : The size of the batches to generate.\n            shuffle_images         : If True, shuffles the images read from the DATASET_PATH\n            image_min_side         : After resizing the minimum side of an image is equal to image_min_side.\n        \"\"\"\n\n        self.batch_size = BATCH_SIZE\n        self.shuffle_images = shuffle_images\n        self.image_min_side = image_min_side\n        self.load_image_paths_labels(DATASET_PATH)\n        self.create_image_groups()\n   \n\n    def load_image_paths_labels(self, DATASET_PATH):\n        \n        self.image_paths = DATASET_PATH[\"Train_image\"]\n        self.image_labels = DATASET_PATH[\"Train_label\"]\n\n        data_out = [0,1,2,3,4]\n        label_int =  dict((c, i) for i, c in enumerate(data_out))\n        int_labe = dict((i, c) for i, c in enumerate(data_out))\n        interger_encoded = [label_int[char] for char in self.image_labels]\n\n        onehot_encoded = []\n        for value in interger_encoded:\n            labe_n = [0 for _ in range(len(data_out))]\n            labe_n[value] = 1\n            onehot_encoded.append(labe_n)\n        train_onehot_labe = onehot_encoded\n    \n        self.image_labels =  train_onehot_labe \n        \n        assert len(self.image_paths) == len(self.image_labels)\n        \n    def create_image_groups(self):\n        self.image_paths = np.asarray(self.image_paths)\n        self.image_labels = np.asarray(self.image_labels)\n        if self.shuffle_images:\n            # Randomly shuffle dataset\n            seed = 4321\n            np.random.seed(seed)\n            np.random.shuffle(self.image_paths)\n            np.random.seed(seed)\n            np.random.shuffle(self.image_labels)\n\n\n        # Divide image_paths and image_labels into groups of BATCH_SIZE\n        self.image_groups = [[self.image_paths[x % len(self.image_paths)] for x in range(i, i + self.batch_size)]\n                              for i in range(0, len(self.image_paths), self.batch_size)]\n        self.label_groups = [[self.image_labels[x % len(self.image_labels)] for x in range(i, i + self.batch_size)]\n                              for i in range(0, len(self.image_labels), self.batch_size)]\n        \n    def resize_image(self, img, min_side_len):\n\n        h, w, c = img.shape\n\n        # limit the min side maintaining the aspect ratio\n        if min(h, w) < min_side_len:\n            im_scale = float(min_side_len) / h if h < w else float(min_side_len) / w\n        else:\n            im_scale = 1.\n\n        new_h = int(h * im_scale)\n        new_w = int(w * im_scale)\n\n        re_im = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n        return re_im, new_h / h, new_w / w\n  \n\n    def load_images(self, image_group):\n\n        images = []\n        for image_path in image_group:\n            img = cv2.imread(image_path)\n            img_shape = len(img.shape)\n            if img_shape == 3:\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img, rh, rw = self.resize_image(img, self.image_min_side)\n            images.append(img)\n        return images\n  \n\n    def construct_image_batch(self, image_group):\n        # get the max image shape\n        max_shape = tuple(max(image.shape[x] for image in image_group) for x in range(3))\n\n        # construct an image batch object\n        image_batch = np.zeros((self.batch_size,) + max_shape, dtype='float32')\n\n        # copy all images to the upper left part of the image batch object\n        for image_index, image in enumerate(image_group):\n            image_batch[image_index, :image.shape[0], :image.shape[1], :image.shape[2]] = image\n\n        return image_batch\n   \n\n    def __len__(self):\n        \"\"\"\n        Number of batches for generator.\n        \"\"\"\n\n        return len(self.image_groups)\n \n\n    def __getitem__(self, index):\n        \"\"\"\n        Keras sequence method for generating batches.\n        \"\"\"\n        image_group = self.image_groups[index]\n        label_group = self.label_groups[index]\n        images = self.load_images(image_group)\n        image_batch = self.construct_image_batch(images)\n\n        return np.array(image_batch), np.array(label_group)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00017-6ee5c53f-0f3d-4089-898c-b128f7ba875c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "864c5146",
        "execution_start": 1617405489931,
        "execution_millis": 1,
        "deepnote_cell_type": "code"
      },
      "source": "def train(model, train_generator, val_generator, epochs = 50):\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n                    loss='categorical_crossentropy',\n                    metrics=['accuracy'])\n\n  \n    history = model.fit_generator(generator=train_generator,\n                                    steps_per_epoch=len(train_generator),\n                                    epochs=epochs,\n                                    validation_data=val_generator,\n                                    validation_steps=len(val_generator))\n\n    return history",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00018-01f17d95-8fd4-46d1-8a76-a165e2a8cd4e",
        "deepnote_cell_type": "code"
      },
      "source": "def run():\n    \n    if __name__ == \"__main__\":\n    \n        # Create FCN model\n        model = FCN_model(len_classes=5, dropout_rate=0.2)\n\n        # The below folders are created using utils.py\n        train_dir = train_data\n        val_dir = Valida_dataset\n    \n        # If you get out of memory error try reducing the batch size\n        BATCH_SIZE=8\n        train_generator = Generator(train_dir, BATCH_SIZE, shuffle_images=True, image_min_side=24)\n        val_generator = Generator(val_dir, BATCH_SIZE, shuffle_images=True, image_min_side=24)\n\n        EPOCHS=50\n        history = train(model, train_generator, val_generator, epochs=EPOCHS)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00018-0e4dc09a-9d6a-47ba-a496-39b05fb76184",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00019-e6459c31-d639-4ab0-ab7e-258b86eab4d6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e3b8005b-8f5d-4e6f-98ca-740c88479b4d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "795b7a63-1767-400b-b646-c33105397c7f",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}