{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00000-8d2d1640-e68e-48a8-b1f0-6e34b775ab42",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "41538265",
        "execution_start": 1617405619491,
        "execution_millis": 5,
        "deepnote_cell_type": "code"
      },
      "source": "from tensorflow.keras.models import Sequential \nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator ## NN \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport skimage.io\nfrom skimage import color\nfrom skimage import io\nimport os\nimport pandas as pd\nimport glob\nimport cv2\nfrom PIL import Image",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-2029e888f3d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPooling2D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImageDataGenerator\u001b[0m \u001b[0;31m## NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00001-03449f33-ef87-4867-8eba-a274608619a0",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "470dc5d0",
        "execution_millis": 9,
        "execution_start": 1617405619833,
        "deepnote_cell_type": "code"
      },
      "source": "main_dir = \"/work/Data\"\ntrain_dir = os.path.join(main_dir,\"/work/Data/train_data\")\n# test_dir = os.path.join(main_dir,\"test_images\")\ntrain_data_image = os.listdir(train_dir)\ntest_data_image = os.listdir(test_dir)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5e53bedd04d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/work/Data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/work/Data/train_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# test_dir = os.path.join(main_dir,\"test_images\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_data_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_data_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "# The clinicians have rated each image for the severity of diabetic retinopathy on a scale of 0 to 4. It is a multi-class problem with 5 target classes\n0 - No DR, 1 - Mild, 2 - Moderate 3 - Severe, 4 - Proliferative DR",
      "metadata": {
        "tags": [],
        "cell_id": "00002-91e6bb59-af55-41e2-a719-82e2232a19dd",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00002-039abb45-be31-4afa-8193-94c11ba2daa1",
        "deepnote_cell_type": "code"
      },
      "source": "Train_data = pd.read_csv(\"../input/aptos2019-blindness-detection/train.csv\")\nTest_data = pd.read_csv(\"../input/aptos2019-blindness-detection/test.csv\")\ntrain_0 = np.count_nonzero(Train_data[\"diagnosis\"] == 0)\ntrain_1 = np.count_nonzero(Train_data[\"diagnosis\"] == 1)\ntrain_2 = np.count_nonzero(Train_data[\"diagnosis\"] == 2)\ntrain_3 = np.count_nonzero(Train_data[\"diagnosis\"] == 3)\ntrain_4 = np.count_nonzero(Train_data[\"diagnosis\"] == 4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00004-866110b9-3b00-4870-b3a7-85a4cdf2c8c3",
        "deepnote_cell_type": "code"
      },
      "source": "# data split\nfrom sklearn.model_selection import train_test_split\n# load image and convert to and from NumPy array\nfrom PIL import Image\nfrom numpy import argmax\nimport numpy as np\nimport cv2\nfrom PIL import Image\ntrain_x = Train_data[\"id_code\"]\ntrain_labels = Train_data[\"diagnosis\"]\n\ntrain_pic_path= [os.path.join(train_dir, filename)for filename in train_x]\ntrain_image = []\nfor i, img_path in enumerate(train_pic_path):\n    img_path = img_path + \".png\"\n    train_image.append(img_path)\nprint(len(train_image))\n\ndata_out = [0,1,2,3,4]\nlabel_int =  dict((c, i) for i, c in enumerate(data_out))\nint_labe = dict((i, c) for i, c in enumerate(data_out))\ninterger_encoded = [label_int[char] for char in train_labels]\n\nonehot_encoded = []\nfor value in interger_encoded:\n    labe_n = [0 for _ in range(len(data_out))]\n    labe_n[value] = 1\n    onehot_encoded.append(labe_n)\ntrain_onehot_labe = onehot_encoded\n\ndata_frame_work = pd.DataFrame({\"Train_label\": train_onehot_labe,\n                               \"Train_image\": train_image,\n                               \"Triain_id\": train_x},\n                              columns=[\"Train_label\",\"Train_image\",\"Triain_id\"])\n\n\nValida_dataset = data_frame_work.sample(n=500)\nTrain_dataset = data_frame_work.drop(Valida_dataset.index)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": " # Image Analysis",
      "metadata": {
        "tags": [],
        "cell_id": "00005-dfab3c8d-7738-4b2d-962a-ce31058f8812",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00006-d7402e75-3557-41c8-ab1b-8bd0626582e2",
        "deepnote_cell_type": "code"
      },
      "source": "\ndef image_analysis(dataframe, path):\n    width_range = []\n    height_range = []\n    for i in range(dataframe.shape[0]):\n        img = cv2.imread(path+dataframe.iloc[i]['id_code']+'.png')\n        height, width, _ = img.shape\n        width_range.append(width)\n        height_range.append(height)\n    return width_range, height_range\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00007-4a7aca1d-9a00-4c8e-8b91-73ffbb1250f6",
        "deepnote_cell_type": "code"
      },
      "source": "width_range, height_range = image_analysis(Train_data, \"../input/aptos2019-blindness-detection/train_images/\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00008-f0a100c9-2cff-45bd-a5b8-d4eccde92b22",
        "deepnote_cell_type": "code"
      },
      "source": "# check image ratio of width and height\nimport seaborn as sns\nratio_list = []\n\nfor i,j in zip(width_range,height_range):\n    ratio = i/j\n    ratio_list.append(ratio)\n\nsns.distplot(ratio_list, hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00009-6b19ab54-7cc6-4075-b02c-2422866d8957",
        "deepnote_cell_type": "code"
      },
      "source": "hh = max(height_range)\nww = max(width_range)\ncc = 3\ncolor = (0,0,0) # add 0 which is the same with backgroup\nresult = np.full((hh,ww,cc), color, dtype=np.uint8)\nnew_image = []\ndef pad_max(image_group): \n    '''\n    image_group: list of image path\n    '''\n    for image in image_group:\n        imge = cv2.imread(image)\n        ht, wd, cc = imge.shape\n        \n        # compute center offset\n        xx = (ww - wd) // 2\n        yy = (hh - ht) // 2\n        \n        # copy img image into center of result image\n        result[yy:yy+ht, xx:xx+wd] = imge\n        \n        new_image.append(imge)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00010-1addb604-79dd-4be2-a60b-98f515e4ae77",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b8ac4fcd",
        "execution_millis": 2498,
        "execution_start": 1617397179698,
        "deepnote_cell_type": "code"
      },
      "source": "train_data = !ls /datasets/trainimage \n",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00012-044cc23e-c664-430a-97e7-2fd56baea64e",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "1c986a8a",
        "execution_start": 1617397200504,
        "execution_millis": 5,
        "deepnote_cell_type": "code"
      },
      "source": "# width, height check\ndef image_analysis(dataframe, path): \n    width_range = []\n    height_range = []\n    for i in range(dataframe.shape[0]):\n        img = cv2.imread(path+dataframe.iloc[i]['id_code']+'.png')\n        height, width, _ = img.shape\n        width_range.append(width)\n        height_range.append(height)\n    return width_range, height_range\n\ndef wid_heigh(image_group):\n    width_range = []\n    height_range = []\n    for i in image_group:\n        height, width, _ = i.shape\n        width_range.append(width)\n        height_range.append(height)\n    return width_range, height_range \n\n# Check all png have the same size\ndef ckeckList(lst):\n  \n    ele = lst[0]\n    chk = True\n      \n    # Comparing each element with first item \n    for item in lst:\n        if ele != item:\n            chk = False\n            break;\n              \n    if (chk == True): print(\"Equal\")\n    else: print(\"Not equal\")   \n        \ndef pad_max(image_group): \n    '''\n    image_group: list of image path\n    '''\n    for image in image_group:\n        imge = cv2.imread(image)\n        ht, wd, cc = imge.shape\n        \n        # compute center offset\n        xx = (ww - wd) // 2\n        yy = (hh - ht) // 2\n        \n        # copy img image into center of result image\n        result[yy:yy+ht, xx:xx+wd] = imge\n        \n        new_image.append(result)\n        ",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "3663"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00013-f3a1b6b4-9e62-4db5-9125-bc7a78e22086",
        "deepnote_cell_type": "code"
      },
      "source": "width_range, height_range = image_analysis(Train_data, \"../input/aptos2019-blindness-detection/train_images/\")\n# check image ratio of width and height\nimport seaborn as sns\nratio_list = []\n\nfor i,j in zip(width_range,height_range):\n    ratio = i/j\n    ratio_list.append(ratio)\n\nsns.distplot(ratio_list, hist=True, kde=True, \n             bins=int(180/5), color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\n    \n# add 0 to images to make it same size\nhh = max(height_range)\nww = max(width_range)\ncc = 3\ncolor = (0,0,0) # add 0 which is the same with backgroup\nresult = np.full((hh,ww,cc), color, dtype=np.uint8)\nnew_image = []\n  \n# check results \nimage_list = Train_dataset[\"Train_image\"][1:200]\npad_max(image_list)\nwidth_list, height_list  = wid_heigh(new_image)\nlst1 = height_list\nckeckList(lst1)\nlst2 = width_list\nckeckList(lst2)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Model Part",
      "metadata": {
        "tags": [],
        "cell_id": "00013-69b0df47-7bf0-4228-8e2e-f9d1fd68c6f5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00014-02a4dd5a-759e-438f-b69f-de3c6d47ce3e",
        "deepnote_cell_type": "code"
      },
      "source": "import tensorflow as tf\n\ndef FCN_model(len_classes=5, dropout_rate=0.2):\n    \n    input = tf.keras.layers.Input(shape=(None, None, 3))\n\n    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1)(input)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # x = tf.keras.layers.MaxPooling2D()(x)\n\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=1)(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # x = tf.keras.layers.MaxPooling2D()(x)\n\n    x = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2)(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # x = tf.keras.layers.MaxPooling2D()(x)\n\n    x = tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=2)(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # x = tf.keras.layers.MaxPooling2D()(x)\n\n    x = tf.keras.layers.Conv2D(filters=512, kernel_size=3, strides=2)(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # Uncomment the below line if you're using dense layers\n    # x = tf.keras.layers.GlobalMaxPooling2D()(x)\n\n    # Fully connected layer 1\n    # x = tf.keras.layers.Dropout(dropout_rate)(x)\n    # x = tf.keras.layers.BatchNormalization()(x)\n    # x = tf.keras.layers.Dense(units=64)(x)\n    # x = tf.keras.layers.Activation('relu')(x)\n\n    # Fully connected layer 1\n    x = tf.keras.layers.Conv2D(filters=64, kernel_size=1, strides=1)(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Activation('relu')(x)\n\n    # Fully connected layer 2\n    # x = tf.keras.layers.Dropout(dropout_rate)(x)\n    # x = tf.keras.layers.BatchNormalization()(x)\n    # x = tf.keras.layers.Dense(units=len_classes)(x)\n    # predictions = tf.keras.layers.Activation('softmax')(x)\n\n    # Fully connected layer 2\n    x = tf.keras.layers.Conv2D(filters=len_classes, kernel_size=1, strides=1)(x)\n    x = tf.keras.layers.Dropout(dropout_rate)(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.GlobalMaxPooling2D()(x)\n    predictions = tf.keras.layers.Activation('softmax')(x)\n\n    model = tf.keras.Model(inputs=input, outputs=predictions)\n    \n    print(model.summary())\n    print(f'Total number of layers: {len(model.layers)}')\n\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Generator Part",
      "metadata": {
        "tags": [],
        "cell_id": "00015-b3d55d1c-5bd8-4f15-a843-4995e6724085",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00015-2378e663-65be-46a6-99b4-0cf5208207ae",
        "deepnote_cell_type": "code"
      },
      "source": "class Generator(tf.keras.utils.Sequence):\n\n    def __init__(self, DATASET_PATH, BATCH_SIZE=32, shuffle_images=True, image_min_side=24):\n        \"\"\" Initialize Generator object.\n        Args \n            DATASET_PATH           : Path to data dataframe\n            BATCH_SIZE             : The size of the batches to generate.\n            shuffle_images         : If True, shuffles the images read from the DATASET_PATH\n            image_min_side         : After resizing the minimum side of an image is equal to image_min_side.\n        \"\"\"\n\n        self.batch_size = BATCH_SIZE\n        self.shuffle_images = shuffle_images\n        self.image_min_side = image_min_side\n        self.load_image_paths_labels(DATASET_PATH)\n        self.create_image_groups()\n   \n\n    def load_image_paths_labels(self, DATASET_PATH):\n        \n        self.image_paths = DATASET_PATH[\"Train_image\"]\n        self.image_labels = DATASET_PATH[\"Train_label\"]\n\n        data_out = [0,1,2,3,4]\n        label_int =  dict((c, i) for i, c in enumerate(data_out))\n        int_labe = dict((i, c) for i, c in enumerate(data_out))\n        interger_encoded = [label_int[char] for char in self.image_labels]\n\n        onehot_encoded = []\n        for value in interger_encoded:\n            labe_n = [0 for _ in range(len(data_out))]\n            labe_n[value] = 1\n            onehot_encoded.append(labe_n)\n        train_onehot_labe = onehot_encoded\n    \n        self.image_labels =  train_onehot_labe \n        \n        assert len(self.image_paths) == len(self.image_labels)\n        \n    def create_image_groups(self):\n        self.image_paths = np.asarray(self.image_paths)\n        self.image_labels = np.asarray(self.image_labels)\n        if self.shuffle_images:\n            # Randomly shuffle dataset\n            seed = 4321\n            np.random.seed(seed)\n            np.random.shuffle(self.image_paths)\n            np.random.seed(seed)\n            np.random.shuffle(self.image_labels)\n\n\n        # Divide image_paths and image_labels into groups of BATCH_SIZE\n        self.image_groups = [[self.image_paths[x % len(self.image_paths)] for x in range(i, i + self.batch_size)]\n                              for i in range(0, len(self.image_paths), self.batch_size)]\n        self.label_groups = [[self.image_labels[x % len(self.image_labels)] for x in range(i, i + self.batch_size)]\n                              for i in range(0, len(self.image_labels), self.batch_size)]\n        \n    def resize_image(self, img, min_side_len):\n\n        h, w, c = img.shape\n\n        # limit the min side maintaining the aspect ratio\n        if min(h, w) < min_side_len:\n            im_scale = float(min_side_len) / h if h < w else float(min_side_len) / w\n        else:\n            im_scale = 1.\n\n        new_h = int(h * im_scale)\n        new_w = int(w * im_scale)\n\n        re_im = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n        return re_im, new_h / h, new_w / w\n  \n\n    def load_images(self, image_group):\n\n        images = []\n        for image_path in image_group:\n            img = cv2.imread(image_path)\n            img_shape = len(img.shape)\n            if img_shape == 3:\n                img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img, rh, rw = self.resize_image(img, self.image_min_side)\n            images.append(img)\n        return images\n  \n\n    def construct_image_batch(self, image_group):\n        # get the max image shape\n        max_shape = tuple(max(image.shape[x] for image in image_group) for x in range(3))\n\n        # construct an image batch object\n        image_batch = np.zeros((self.batch_size,) + max_shape, dtype='float32')\n\n        # copy all images to the upper left part of the image batch object\n        for image_index, image in enumerate(image_group):\n            image_batch[image_index, :image.shape[0], :image.shape[1], :image.shape[2]] = image\n\n        return image_batch\n   \n\n    def __len__(self):\n        \"\"\"\n        Number of batches for generator.\n        \"\"\"\n\n        return len(self.image_groups)\n \n\n    def __getitem__(self, index):\n        \"\"\"\n        Keras sequence method for generating batches.\n        \"\"\"\n        image_group = self.image_groups[index]\n        label_group = self.label_groups[index]\n        images = self.load_images(image_group)\n        image_batch = self.construct_image_batch(images)\n\n        return np.array(image_batch), np.array(label_group)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00017-6ee5c53f-0f3d-4089-898c-b128f7ba875c",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "864c5146",
        "execution_start": 1617405489931,
        "execution_millis": 1,
        "deepnote_cell_type": "code"
      },
      "source": "def train(model, train_generator, val_generator, epochs = 50):\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n                    loss='categorical_crossentropy',\n                    metrics=['accuracy'])\n\n  \n    history = model.fit_generator(generator=train_generator,\n                                    steps_per_epoch=len(train_generator),\n                                    epochs=epochs,\n                                    validation_data=val_generator,\n                                    validation_steps=len(val_generator))\n\n    return history",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00018-01f17d95-8fd4-46d1-8a76-a165e2a8cd4e",
        "deepnote_cell_type": "code"
      },
      "source": "def run():\n    \n    if __name__ == \"__main__\":\n    \n        # Create FCN model\n        model = FCN_model(len_classes=5, dropout_rate=0.2)\n\n        # The below folders are created using utils.py\n        train_dir = train_data\n        val_dir = Valida_dataset\n    \n        # If you get out of memory error try reducing the batch size\n        BATCH_SIZE=8\n        train_generator = Generator(train_dir, BATCH_SIZE, shuffle_images=True, image_min_side=24)\n        val_generator = Generator(val_dir, BATCH_SIZE, shuffle_images=True, image_min_side=24)\n\n        EPOCHS=50\n        history = train(model, train_generator, val_generator, epochs=EPOCHS)",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00018-0e4dc09a-9d6a-47ba-a496-39b05fb76184",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "",
      "metadata": {
        "tags": [],
        "cell_id": "00019-e6459c31-d639-4ab0-ab7e-258b86eab4d6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=e3b8005b-8f5d-4e6f-98ca-740c88479b4d' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "orig_nbformat": 2,
    "deepnote_notebook_id": "795b7a63-1767-400b-b646-c33105397c7f",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}