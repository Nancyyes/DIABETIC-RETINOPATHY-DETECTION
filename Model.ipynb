{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpVi0JroIPmw"
      },
      "source": [
        "# load image and convert to and from NumPy array\n",
        "from PIL import Image\n",
        "from numpy import argmax\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "train_pic_path= [os.path.join(train_dir, filename)for filename in train_x]\n",
        "train_image = []\n",
        "for i, img_path in enumerate(train_pic_path):\n",
        "    img_path = img_path + \".png\"\n",
        "    img = Image.open(img_path)\n",
        "    train_image.append(img)\n",
        "print(len(train_image))\n",
        "\n",
        "data_out = [0,1,2,3,4]\n",
        "label_int =  dict((c, i) for i, c in enumerate(data_out))\n",
        "int_labe = dict((i, c) for i, c in enumerate(data_out))\n",
        "interger_encoded = [label_int[char] for char in train_labels]\n",
        "\n",
        "onehot_encoded = []\n",
        "for value in interger_encoded:\n",
        "    labe_n = [0 for _ in range(len(data_out))]\n",
        "    labe_n[value] = 1\n",
        "    onehot_encoded.append(labe_n)\n",
        "train_onehot_labe = onehot_encoded\n",
        "\n",
        "data_frame_work = pd.DataFrame({\"Train_label\": train_onehot_labe,\n",
        "                               \"Train_image\": train_image,\n",
        "                               \"Triain_id\": train_x},\n",
        "                              columns=[\"Train_label\",\"Train_image\",\"Triain_id\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWYa953LIGQg"
      },
      "source": [
        "# CNN\n",
        "## Need to standard image size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBLX_MV4IEns"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(filters=16, kernel_size=(2, 2), input_shape=[512,512,3], activation= 'relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(filters=32, kernel_size=(2, 2), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(filters=64, kernel_size=(2, 2), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(units=128, activation = 'relu'))\n",
        "model.add(Dense(units=256, activation = 'relu'))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dense(5, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.00005), metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tgDtSCLIqNj"
      },
      "source": [
        "# FCN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBQZ-jTiKqwy"
      },
      "source": [
        "# To solve the unstandard size images, find out the highest, widest image, and add 0 to others to make them become same size.By doing so, we still can pass it to fit_generator()\n",
        "def image_batch(image_group, BATCH_SIZE):\n",
        "    # get the max image shape\n",
        "    max_shape = tuple(max(image.shape[x] for image in image_group) for x in range(3))\n",
        "\n",
        "    # construct an image batch object\n",
        "    image_batch = np.zeros((BATCH_SIZE,) + max_shape, dtype='float32')\n",
        "\n",
        "    # copy all images to the upper left part of the image batch object\n",
        "    for image_index, image in enumerate(image_group):\n",
        "        image_batch[image_index, :image.shape[0], :image.shape[1], :image.shape[2]] = image\n",
        "\n",
        "    return image_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjQ2-F-2IzA1"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def FCN_model(len_classes=5, dropout_rate=0.2):\n",
        "    \n",
        "    # Input layer\n",
        "    input = tf.keras.layers.Input(shape=(None, None, 3)) # because we don't have a standarded so set as None, None to represent width, height\n",
        "\n",
        "    # A convolution block\n",
        "    x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=1)(input)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    \n",
        "    # Fully connected layer 1\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=1, strides=1)(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "    # Fully connected layer 2\n",
        "    x = tf.keras.layers.Conv2D(filters=64, kernel_size=1, strides=1)(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('relu')(x)\n",
        "    \n",
        "    model = tf.keras.Model(inputs=input, outputs=predictions)\n",
        "    print(model.summary())"
      ],
      "execution_count": 1,
      "outputs": []
    }
  ]
}